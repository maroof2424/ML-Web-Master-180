

### ğŸ§  Week 11 â€“ Deep Learning Intro (TensorFlow / Keras)

**ğŸ“ Directory:** `Month3_ML_DL/Week11_TensorFlow`

#### ğŸ—‚ Files:

```
01_setup_tensorflow.ipynb
02_first_ann.ipynb
03_loss_activation.ipynb
04_binary_classification_ann.ipynb
05_multiclass_ann.ipynb
06_mini_project_ann.ipynb
README.md
```

---

## ğŸ¯ Learning Objective

This week, youâ€™ll understand the **core concepts of Artificial Neural Networks (ANN)** and learn to **build and train neural models using TensorFlow & Keras** â€” even without a GPU (by using Google Colab).

---

## ğŸ§© Day-wise Plan

### **Day 1: TensorFlow / Keras Setup**

* Install TensorFlow and Keras.
* Run a test model to confirm setup.
* Check GPU availability in Colab using:

  ```python
  import tensorflow as tf
  print("TensorFlow version:", tf.__version__)
  print("GPU Available:", tf.config.list_physical_devices('GPU'))
  ```
* âœ… Tip: If GPU isnâ€™t listed, go to **Runtime â†’ Change runtime type â†’ GPU**.

---

### **Day 2: ANN Basics**

* Learn about:

  * Neurons, layers, weights, biases.
  * Forward propagation & backpropagation (conceptually).
* Build your **first manual ANN** with dense layers using Keras.

---

### **Day 3: Loss & Activation Functions**

* Understand:

  * **Activation functions:** `sigmoid`, `relu`, `softmax`, `tanh`.
  * **Loss functions:** `binary_crossentropy`, `categorical_crossentropy`, `mse`.
* Experiment with different activation + loss combinations.

---

### **Day 4: Binary Classification ANN**

* Dataset ideas: **Titanic**, **Spam detection**, or **Loan Approval**.
* Build an ANN with:

  * Input â†’ Hidden (ReLU) â†’ Output (Sigmoid)
* Evaluate accuracy, precision, recall.

---

### **Day 5: Multi-Class ANN**

* Use **small MNIST / Fashion MNIST** dataset.
* Implement:

  * Output layer â†’ `Softmax`
  * Loss â†’ `categorical_crossentropy`
* Plot training/validation accuracy.

---

### **Day 6: Mini Project â€“ Binary Classification**

* Choose dataset:

  * Fake News detection
  * Customer churn prediction
  * Loan default prediction
* Train â†’ Evaluate â†’ Save model using:

  ```python
  model.save('my_ann_model.h5')
  ```

---

### **Day 7: Review & Wrap-Up**

* Compare ANN vs classical ML (SVM, RF).
* Summarize:

  * Why activation matters
  * Gradient descent concept
  * When to use ANN over ML
* Backup notebooks to GitHub or Google Drive.

---

## âš™ï¸ Environment Notes

| Platform           | Recommended                | Notes                 |
| ------------------ | -------------------------- | --------------------- |
| ğŸ’» Local PC        | âŒ Not recommended (no GPU) | Training will be slow |
| â˜ï¸ Google Colab    | âœ… Recommended              | Free GPU available    |
| ğŸ§® Kaggle Notebook | âœ… Optional                 | Also provides GPU     |

---

## ğŸ§° Dependencies

Install inside Colab cell:

```bash
!pip install tensorflow numpy pandas scikit-learn matplotlib
```

---

## ğŸ§  Key Concepts Recap

* ANN = layers of neurons (input â†’ hidden â†’ output)
* Weights adjust via **backpropagation**
* Optimization via **Gradient Descent**
* Loss + Activation determine model learning
* GPU = faster matrix operations (optional but ideal)

---

## ğŸ’¡ Bonus Challenge

Try converting your sklearn model (e.g., Logistic Regression) into a **Keras Sequential ANN** and compare:

* Accuracy
* Training time
* Model size

---