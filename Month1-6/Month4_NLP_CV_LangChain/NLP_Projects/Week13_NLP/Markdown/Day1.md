
# üß†Day 1: Text Preprocessing
# -------------------------------------

# Install required packages (if not already installed)
```
!pip install nltk
```
# Import libraries
```
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
```
# Download NLTK data (only first time)
```
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

# üìÑ Sample text
```
text = "Natural Language Processing (NLP) helps computers understand human language. It's awesome!"

print("üîπ Original Text:\n", text)
```

---

### Step 1Ô∏è‚É£: Lowercasing and Removing Punctuation

```python
# Lowercase
text_lower = text.lower()

# Remove punctuation
text_clean = "".join([ch for ch in text_lower if ch not in string.punctuation])

print("‚úÖ Cleaned Text:\n", text_clean)
```

---

### Step 2Ô∏è‚É£: Tokenization

```python
tokens = word_tokenize(text_clean)
print("üß© Tokens:\n", tokens)
```

---

### Step 3Ô∏è‚É£: Remove Stopwords

```python
stop_words = set(stopwords.words('english'))
filtered_tokens = [w for w in tokens if w not in stop_words]

print("üö´ After Stopword Removal:\n", filtered_tokens)
```

---

### Step 4Ô∏è‚É£: Stemming (Reduce to Root Form)

```python
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

print("üå± After Stemming:\n", stemmed_tokens)
```

---

### Step 5Ô∏è‚É£: Lemmatization (Better than Stemming)

```python
lemmatizer = WordNetLemmatizer()
lemm_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

print("üß† After Lemmatization:\n", lemm_tokens)
```

---

### üßæ Combine Steps into a Function

```python
def preprocess_text(text):
    text = text.lower()
    text = "".join([ch for ch in text if ch not in string.punctuation])
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return tokens

sample = "NLTK is a powerful Python library for text processing!"
print("‚úÖ Final Output:", preprocess_text(sample))
```

---

### üìä Output Example:

```
üîπ Original Text:
Natural Language Processing (NLP) helps computers understand human language. It's awesome!

‚úÖ Cleaned Text:
natural language processing nlp helps computers understand human language its awesome

üß© Tokens:
['natural', 'language', 'processing', 'nlp', 'helps', 'computers', 'understand', 'human', 'language', 'its', 'awesome']

üö´ After Stopword Removal:
['natural', 'language', 'processing', 'nlp', 'helps', 'computers', 'understand', 'human', 'language', 'awesome']

üå± After Stemming:
['natur', 'languag', 'process', 'nlp', 'help', 'comput', 'understand', 'human', 'languag', 'awesom']

üß† After Lemmatization:
['natural', 'language', 'processing', 'nlp', 'help', 'computer', 'understand', 'human', 'language', 'awesome']
```

---

### üí° Summary

| Step               | Purpose           | Library             |
| ------------------ | ----------------- | ------------------- |
| Lowercasing        | Normalize text    | Python built-in     |
| Remove punctuation | Remove noise      | `string`            |
| Tokenization       | Split into words  | `nltk.tokenize`     |
| Stopword removal   | Drop common words | `nltk.stopwords`    |
| Stemming           | Crude root        | `PorterStemmer`     |
| Lemmatization      | Smart root        | `WordNetLemmatizer` |

---
