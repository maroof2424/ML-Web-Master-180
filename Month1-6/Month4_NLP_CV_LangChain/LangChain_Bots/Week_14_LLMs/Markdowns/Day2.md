

# ğŸ§  Day 2 â€” LangChain Chains & Memory

### ğŸ¯ **Goal**

Learn how to:

* Build **sequential chains** (multi-step workflows)
* Use **conversation memory** so your chatbot â€œremembersâ€ context

---

## ğŸš€ Step 1: What Are Chains?

In LangChain, a **Chain** means a **pipeline of components** â€”
you give an input, it passes through prompts â†’ LLM â†’ output parsers â†’ memory.

**Example:**

```
User Question â†’ Prompt â†’ LLM â†’ Output
```

A **Sequential Chain** = Multiple chains connected in sequence.

```
Input â†’ Step1: Summarize â†’ Step2: Explain â†’ Step3: Answer
```

---

## âš™ï¸ Step 2: Memory in LangChain

Memory allows your LLM to **remember previous interactions**.

### ğŸ§© Types of Memory

| Type                             | Description                         | Use Case       |
| -------------------------------- | ----------------------------------- | -------------- |
| `ConversationBufferMemory`       | Stores last chat messages           | Short sessions |
| `ConversationSummaryMemory`      | Summarizes old chats to save memory | Long chats     |
| `ConversationBufferWindowMemory` | Keeps only the last N messages      | Medium chats   |

---

## ğŸ’¡ Step 3: Setup Environment

### ğŸ§° Required Installs

```bash
pip install langchain
pip install langchain-google-genai
pip install google-generativeai==0.5.4
pip install python-dotenv
```

> âš ï¸ Use version `google-generativeai==0.5.4` to avoid conflicts.

---

## ğŸ” Step 4: Setup `.env`

Create a `.env` file in your project folder:

```bash
GOOGLE_API_KEY=your_actual_google_api_key_here
```

---

## ğŸ’» Step 5: Example Code â€” Gemini + Memory

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv
import os

# Load API key
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Initialize LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.7,
    google_api_key=GOOGLE_API_KEY
)

# Initialize Memory
memory = ConversationBufferMemory(memory_key="chat_history")

# Create Prompt Template
prompt = ChatPromptTemplate.from_template("""
You are a helpful AI assistant.
Conversation so far:
{chat_history}
User Question:
{question}
""")

# Output Parser
parser = StrOutputParser()

# Combine into a chain
chain = prompt | llm | parser

# Conversation flow
user_inputs = [
    "Hello! Who created LangChain?",
    "Explain it simply.",
    "Give me an example?"
]

for question in user_inputs:
    response = chain.invoke({
        "question": question,
        "chat_history": memory.load_memory_variables({})["chat_history"]
    })
    
    print(f"\nğŸ§‘ User: {question}")
    print(f"ğŸ¤– Bot: {response}")
    
    # Save in memory
    memory.save_context({"question": question}, {"answer": response})
```

---

## ğŸ§  Step 6: How It Works

1. `ConversationBufferMemory` stores previous chat.
2. Each new question adds to the memory.
3. The `prompt` always includes `{chat_history}` so the LLM â€œremembersâ€.
4. You can replace memory with `ConversationSummaryMemory` for long sessions.

---

## ğŸ§© Step 7: Bonus â€” Sequential Chain (Multi-Step Example)

```python
from langchain.chains import SequentialChain
from langchain_core.prompts import PromptTemplate

# Step 1: Summarize
summary_prompt = PromptTemplate.from_template("Summarize this text:\n{text}")
# Step 2: Make it simpler
explain_prompt = PromptTemplate.from_template("Explain this summary simply:\n{summary}")

chain1 = summary_prompt | llm | parser
chain2 = explain_prompt | llm | parser

seq_chain = SequentialChain(
    chains=[chain1, chain2],
    input_variables=["text"],
    output_variables=["summary", "explanation"]
)

output = seq_chain.invoke({"text": "LangChain helps developers build LLM-powered apps using components like chains, memory, and agents."})
print(output)
```

---

## ğŸ§© Output Example

```
ğŸ§‘ User: Hello! Who created LangChain?
ğŸ¤– Bot: LangChain was created by Harrison Chase in 2022.

ğŸ§‘ User: Explain it simply.
ğŸ¤– Bot: It's a Python library that helps connect AI models together easily.
```

---

## âœ… Summary

| Concept         | Key Idea                    |
| --------------- | --------------------------- |
| Chain           | A workflow of LLM steps     |
| SequentialChain | Multiple chains in sequence |
| Memory          | Keeps conversation context  |
| BufferMemory    | Saves recent chats          |
| SummaryMemory   | Summarizes old chats        |

---

## ğŸ’¬ Tip of the Day

> â€œAlways add memory to your chatbot â€” bina memory ke AI bas ek calculator jaisa lagta hai ğŸ˜‰â€

---
