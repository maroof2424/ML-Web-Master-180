# ‚úÖ **Day6.md ‚Äî QA Chatbot Project (Knowledge-Based RAG Chatbot)**

**File:** `06_qa_chatbot.ipynb`

---

# üéØ **Aaj ka Goal**

Aaj hum ek **QA Chatbot** banayenge jo:

‚úÖ User se documents lega (PDF, TXT, MD)
‚úÖ Embeddings banayega
‚úÖ Vector store me save karega
‚úÖ Query ko samajh kar relevant text retrieve karega
‚úÖ LLM se final answer generate karayega

Yani full **RAG Chatbot** (Retrieval-Augmented Generation).

Ye real-world project hai ‚Äî companies exactly ‡§Ø‡§π‡•Ä use karti hain internal docs, policies, SOPs, FAQs ke liye.

---

# üß† **1. RAG Chatbot Flow (Simple Urdu + Diagram Style)**

```
User Question
       ‚Üì
Retriever (FAISS)
       ‚Üì relevant docs
LLM (HF / Gemini)
       ‚Üì
Final Answer
```

Agar koi concept ya context missing ho toh **LLM hallucinate nahi karta**, balki FAISS se original lines utha kar correct answer deta.

---

# ‚öôÔ∏è **2. Install Required Packages**

```bash
pip install langchain langchain-core langchain-community
pip install sentence-transformers faiss-cpu
pip install pypdf transformers accelerate
```

---

# üìÇ **3. Step-by-Step Project (Very Clear Flow)**

---

# ‚úÖ **Step 1 ‚Äî Imports**

```python
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
```

---

# ‚úÖ **Step 2 ‚Äî Load Documents**

### **PDF Example**

```python
loader = PyPDFLoader("docs/myfile.pdf")
documents = loader.load()
```

### **Text Example**

```python
loader = TextLoader("docs/notes.txt")
documents = loader.load()
```

---

# ‚úÖ **Step 3 ‚Äî Embeddings Model**

Fast + accurate:

```
all-MiniLM-L6-v2
```

```python
embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

---

# ‚úÖ **Step 4 ‚Äî Convert Docs ‚Üí Vector Store**

```python
db = FAISS.from_documents(documents, embedding)
retriever = db.as_retriever()
```

---

# ‚úÖ **Step 5 ‚Äî LLM Setup (Flan-T5)**

```python
model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=300
)

llm = HuggingFacePipeline(pipeline=pipe)
```

---

# ‚úÖ **Step 6 ‚Äî Build QA Chain (RAG Brain)**

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)
```

---

# ‚úÖ **Step 7 ‚Äî Ask Questions**

```python
query = "What is the main idea of this document?"
response = qa_chain.invoke(query)

print(response["result"])
```

---

# ‚úÖ **Step 8 ‚Äî Loop Chatbot (Optional)**

```python
while True:
    user_q = input("Ask: ")
    if user_q.lower() == "exit":
        break
    ans = qa_chain.invoke(user_q)
    print("\nBot:", ans["result"], "\n")
```

---

# üèóÔ∏è **Full Script (06_qa_chatbot.py)**

```python
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# --- 1) Load Docs
loader = PyPDFLoader("docs/myfile.pdf")
documents = loader.load()

# --- 2) Embeddings
embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# --- 3) Vector Store
db = FAISS.from_documents(documents, embedding)
retriever = db.as_retriever()

# --- 4) LLM
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
llm = HuggingFacePipeline(pipeline=pipe)

# --- 5) QA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

# --- 6) Ask
print(qa_chain.invoke("Summarize this document")["result"])
```

---

# üßë‚Äçüè´ **Mentor-Style Advice**

* Bro, yeh **Day 6 project** tumhare future RAG apps ki backbone hai.
* Ek advice:
  **LLM ko heavy mat banao ‚Äî embeddings strong karo.**
* Documents zyada hue toh FAISS best hai; Chroma bhi try kar sakte ho.
* Aage Agents + Tools + Memory add karke aap **full enterprise-grade chatbot** bana sakte ho.

---

# ‚úÖ **1-Min Summary**

* Document load karo (PDF/TXT)
* Embeddings banao
* FAISS vector store create karo
* Retriever se relevant docs lao
* LLM se final answer generate karo
* Yeh pura workflow = **RAG chatbot**

---
